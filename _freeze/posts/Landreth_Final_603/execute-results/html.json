{
  "hash": "6167ab34d38fb06bff004ce594d6855c",
  "result": {
    "markdown": "---\ntitle: \"Final Project 603\"\nauthor: \"Connor Landreth\"\ndesription: \"How Various Lifestyle & Personal Factors Interact with Client Satisfaction\"\ndate: \"08/15/2023\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n    code-copy: true\n    code-tools: true\ncategories:\n  - Final Project\n  - Connor Landreth\n  - Hours worked\n  - Job performance\n  - ggplot2\n  - tidyverse\neditor: \n  markdown: \n    wrap: sentence\n---\n\n\n### Setup\n\n#### Load in Relevant Packages\n\nFor descriptive analysis, we'll use quite a few classic packages including Tidyverse, ggplot2, knitr, and summarytools\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   1.0.0 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n```\n:::\n\n```{.r .cell-code}\nlibrary(knitr)\nlibrary(kableExtra)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'kableExtra' was built under R version 4.2.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n```\n:::\n\n```{.r .cell-code}\nlibrary(xml2)\nlibrary(psych)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'psych' was built under R version 4.2.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'psych'\n\nThe following objects are masked from 'package:ggplot2':\n\n    %+%, alpha\n```\n:::\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(ggthemes)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'ggthemes' was built under R version 4.2.3\n```\n:::\n\n```{.r .cell-code}\nlibrary(summarytools)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'summarytools' was built under R version 4.2.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'summarytools'\n\nThe following object is masked from 'package:tibble':\n\n    view\n```\n:::\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(reshape2)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'reshape2'\n\nThe following object is masked from 'package:tidyr':\n\n    smiths\n```\n:::\n\n```{.r .cell-code}\nlibrary(SmartEDA)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'SmartEDA' was built under R version 4.2.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n```\n:::\n\n```{.r .cell-code}\n#install.packages(\"summarytools\")\nlibrary(summarytools)\n#install.packages(\"car\")\nlibrary(car)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'car' was built under R version 4.2.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: carData\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'carData' was built under R version 4.2.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:psych':\n\n    logit\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n```\n:::\n\n```{.r .cell-code}\nlibrary(sandwich)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'sandwich' was built under R version 4.2.3\n```\n:::\n\n```{.r .cell-code}\nlibrary(stargazer)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nPlease cite as: \n\n Hlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables.\n R package version 5.2.3. https://CRAN.R-project.org/package=stargazer \n```\n:::\n:::\n\n\n#### Set wd and read in data\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngetwd()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"C:/Users/cjlan/OneDrive/Desktop/603_Summer2023_Sec_1/posts\"\n```\n:::\n\n```{.r .cell-code}\n#JP = Job Performance\nJP <- read.csv(\"Connor_datafolder/JP.csv\")\n\n#View(JP)\n```\n:::\n\n\n### Clean\n\n#### Change variable names then make a descriptive table, make new column\n\nFirst and foremost, there are a few glaring things I want to clean within this data.\nFirst, there are two psych score columns.\nI have no need for two separate columns because it doesn't increase their value.\nInstead, I'll average out the two scores in a new column and delete the original two.\nAs I clean the data, I have the following primary research question in mind: How does quantity of hours worked and quantity of personal development hours affect client satisfaction?\n\nAlso, I'll rename headers to make the data easier to work with.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#head(JP)\n\n# Add psych test 1 & 2 and divide by two, making new average score column\n\nJP$avg_psych_score <- (JP$PsychTest1 + JP$PsychTest2)/2\n#Remove psych tests 1 & 2\nJP <- JP[ ,-1]\nJP <- JP[ ,-1]\n\n#Create new df with renamed variables\nJP_Clean <- JP %>% \n  rename(hrs_personal_dev = \"HrsTrn\",\n         hrs_working = \"HrsWrk\",\n         client_sat = \"ClientSat\",\n         super_sat = \"SuperSat\",\n         success_project_complete = \"ProjCompl\",\n         years_edu = \"YrsEdu\"\n  )\n\nhead(JP_Clean)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  years_edu  IQ hrs_personal_dev hrs_working client_sat super_sat\n1         5  97                6          33         84        59\n2         2  93                7          54         55        38\n3         2  96                5          47         70        68\n4         4 103                7          80         63        81\n5         4  98                5          53         55        39\n6         3 102               10          56         63        68\n  success_project_complete avg_psych_score\n1                       34            70.0\n2                       56            36.5\n3                       38            71.5\n4                       78            55.5\n5                       56            41.5\n6                       57            58.5\n```\n:::\n:::\n\n\n#### Create df with variables, meaning, and measurement scale\n\nThe data is only as impactful as our understanding of it.\nI will create a indexed df below with each variable and its meaning.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create variables\nVariables <- c('years_edu', 'IQ', 'hrs_personal_dev', 'hrs_working', 'client_sat', 'super_sat', 'success_project_complete', 'avg_psych_score')\n# Add meaning for each variable\nMeaning <- c('Years of higher education', 'IQ Test Score', 'Hours dedicated to personal culture, outside of work', 'Hours worked on average per week', 'Customer satisfaction with the worker, score 0-100', 'Satisfaction of the superior with the worker, score 0-100', 'Percentage of projects successfully completed, score 0-100', 'Psychological test, score 0-100 (2 tests)')\n# Add measurement scale for each variable \nMeasurement.Scale <- c('Discrete', 'Continuous', 'Continuous','Continuous', 'Continuous','Continuous', 'Continuous', 'Continuous')\n\n# Join the variables to create a data frame\nIndex_1 <- data.frame(Variables, Meaning, Measurement.Scale)\n#View(Index)\n```\n:::\n\n\n### Summarize\n\n#### Load in summarytools to get stats analysis of variables\n\nThe best way I have found to get a holistic understanding and look at all variables is to use summarytools (only works for numeric variables, luckily all of mine are numeric).\nIt displays mean, std.\ndeviation, max, min, and other measures that may be valuable for our descriptive analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummarytools::descr(JP_Clean)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDescriptive Statistics  \nJP_Clean  \nN: 1000  \n\n                    avg_psych_score   client_sat   hrs_personal_dev   hrs_working        IQ\n----------------- ----------------- ------------ ------------------ ------------- ---------\n             Mean             49.64        54.97               6.03         47.81     97.59\n          Std.Dev             11.40        18.00               2.48         24.08      3.72\n              Min             13.00         0.00               0.00          0.00     90.00\n               Q1             42.00        43.00               4.00         30.00     95.00\n           Median             49.50        55.00               6.00         47.00     97.00\n               Q3             57.00        67.00               8.00         64.00    101.00\n              Max             83.50       100.00              17.00        100.00    105.00\n              MAD             11.12        17.79               2.97         25.20      4.45\n              IQR             15.00        24.00               4.00         34.00      6.00\n               CV              0.23         0.33               0.41          0.50      0.04\n         Skewness             -0.08        -0.05               0.60          0.15     -0.01\n      SE.Skewness              0.08         0.08               0.08          0.08      0.08\n         Kurtosis             -0.03        -0.09               0.72         -0.59     -0.91\n          N.Valid           1000.00      1000.00            1000.00       1000.00   1000.00\n        Pct.Valid            100.00       100.00             100.00        100.00    100.00\n\nTable: Table continues below\n\n \n\n                    success_project_complete   super_sat   years_edu\n----------------- -------------------------- ----------- -----------\n             Mean                      48.13       49.91        2.51\n          Std.Dev                      20.54       17.19        1.72\n              Min                       0.00        0.00        0.00\n               Q1                      33.00       38.00        1.00\n           Median                      48.00       50.00        3.00\n               Q3                      63.00       62.00        4.00\n              Max                     100.00      100.00        5.00\n              MAD                      22.24       17.79        2.22\n              IQR                      30.00       24.00        3.00\n               CV                       0.43        0.34        0.68\n         Skewness                       0.08        0.03       -0.02\n      SE.Skewness                       0.08        0.08        0.08\n         Kurtosis                      -0.54       -0.34       -1.28\n          N.Valid                    1000.00     1000.00     1000.00\n        Pct.Valid                     100.00      100.00      100.00\n```\n:::\n\n```{.r .cell-code}\n#All numeric so all present \n```\n:::\n\n\n#### Plots of client satisfaction vs. hrs worked, hrs personal development & One-Variable Regression\n\nMy personal favorite visualizations - geom_jitter/point to help show the correlation between hours of personal development and customer satisfaction.\nBelow the viz, we'll calculate the correlation and see if there is any significance.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Client sat has a mean of 54.9, so we will filter for the best performers and evaluate their hours worked, then look at all hours worked.\n\nJP_Clean %>% \n    #filter(client_sat >= \"54.9\") %>% \n  ggplot(aes(client_sat,hrs_personal_dev))+\n  geom_jitter(size=2, alpha = 0.5)+\n  #width = x\n  geom_smooth(method = \"lm\")+\n  #facet_wrap(~Gender)+\n  labs(x='Client Satisfaction Score', y='Hours of Personal Development') +\n  theme_linedraw()+\n  labs(title=\"Client Satisfaction vs. Hours of Personal Development \")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](Landreth_Final_603_files/figure-html/CSAT_v_PD-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# NOW LETS LOOK AT HOURS OF PERSONAL DEVELOPMENT AND SEE IF THAT ACTUALLY INCREASES CLIENT SATIFACTION\ncorrelation2 <- cor(JP_Clean$hrs_personal_dev,JP_Clean$client_sat)\n\nprint(correlation2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1546341\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nJP_Clean %>% \n  #filter(hrs_working < 50) %>% \n    #filter(client_sat >= \"54.9\") %>% \n  ggplot(aes(client_sat,hrs_working, color = client_sat))+\n  geom_jitter(size=2, alpha = 0.5)+\n  #width = x\n  geom_smooth(method = \"lm\")+\n  #facet_wrap(~Gender)+\n  labs(x='Client Satisfaction Score', y='Hours Working') +\n  theme_linedraw()+\n  labs(title=\"Client Satisfaction & Hours Worked\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: The following aesthetics were dropped during statistical transformation: colour\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n```\n:::\n\n::: {.cell-output-display}\n![](Landreth_Final_603_files/figure-html/CSAT_v_HW-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# We'll new look at\ncorrelation <- cor(JP_Clean$client_sat, JP_Clean$hrs_working)\n\nprint(correlation)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1107844\n```\n:::\n\n```{.r .cell-code}\nsummary(lm(JP_Clean$client_sat ~ JP_Clean$hrs_working))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = JP_Clean$client_sat ~ JP_Clean$hrs_working)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-56.139 -11.898  -0.161  11.516  45.919 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(>|t|)    \n(Intercept)          51.01020    1.25892  40.519  < 2e-16 ***\nJP_Clean$hrs_working  0.08282    0.02352   3.521 0.000449 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.9 on 998 degrees of freedom\nMultiple R-squared:  0.01227,\tAdjusted R-squared:  0.01128 \nF-statistic:  12.4 on 1 and 998 DF,  p-value: 0.0004485\n```\n:::\n\n```{.r .cell-code}\n# No real correlation, seems more hours worked doesn't necessarily mean those hours were used wisely, as client often seems unhappy as hours increase. \n```\n:::\n\n\nNo real correlation, seems more hours worked doesn't necessarily mean those hours were used wisely, as client often seems unhappy as hours increase.\n\n## Multivariate Regression - Project 2 Starts (New Info)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlin_model_work <- lm(JP_Clean$client_sat ~ JP_Clean$hrs_working + JP_Clean$hrs_personal_dev, data = JP_Clean)\n\n\n#model2 <- lm(JP_Clean$client_sat ~ JP_Clean$IQ + JP_Clean$hrs_personal_dev, data = JP_Clean)\n\nsummary(lin_model_work)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = JP_Clean$client_sat ~ JP_Clean$hrs_working + JP_Clean$hrs_personal_dev, \n    data = JP_Clean)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-57.208 -12.040  -0.575  11.860  46.069 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(>|t|)    \n(Intercept)               48.21875    1.47773  32.630  < 2e-16 ***\nJP_Clean$hrs_working      -0.03462    0.04049  -0.855 0.392702    \nJP_Clean$hrs_personal_dev  1.39456    0.39244   3.554 0.000398 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.8 on 997 degrees of freedom\nMultiple R-squared:  0.02463,\tAdjusted R-squared:  0.02267 \nF-statistic: 12.59 on 2 and 997 DF,  p-value: 3.996e-06\n```\n:::\n:::\n\n\nThe regression model has two predictor variables and one response variable.\nLet's break down the interpretation for each part of the output:\n\nCoefficients: The intercept (represented by (Intercept)) has an estimated value of 48.21875.\nThis is the predicted value of the response variable when all predictor variables are zero.\n\nThe coefficient for the variable hrs_working is -0.03462.\nThis means that for every one unit increase in the hrs_working predictor variable, the response variable (dependent variable) is expected to decrease by approximately 0.03462 units, holding all other variables constant.\nHowever, this coefficient is not statistically significant (p-value = 0.392702 \\> 0.05), which means we cannot conclude that there is a significant linear relationship between hrs_working and the client satisfaction.\n\nThe coefficient for the variable hrs_personal_dev is 1.39456.\nThis means that for every one unit increase in the hrs_personal_dev predictor variable, the response variable is expected to increase by approximately 1.39456 units, holding all other variables constant.\nThat is, work on one more hour of personal development, increase client satisfaction by almost 1.4 points or percent.\nThis coefficient is statistically significant (p-value = 0.000398 \\< 0.05), indicating a significant linear relationship between hrs_personal_dev and the response variable of client satisfaction.\n\nResiduals: The residuals represent the differences between the observed values of the response variable and the predicted values from the regression model.\nThe minimum residual is -57.208, and the maximum residual is 46.069.\nResiduals close to zero indicate that the model is a good fit for the data.\n\nResidual standard error: The residual standard error is a measure of the typical amount that the response variable deviates from the regression model's predicted values.\nIn this case, the residual standard error is approximately 17.8.\n\nMultiple R-squared and Adjusted R-squared: The multiple R-squared (R\\^2) is a measure of how well the regression model explains the variance in the response variable.\nIn this case, the multiple R-squared is 0.02463, which means the model explains about 2.46% of the variance in client satisfaction\n\nThe adjusted R-squared takes into account the number of predictor variables and sample size and is slightly lower at 0.02267.\n\nF-statistic and p-value: The F-statistic is used to test the overall significance of the regression model.\nIn this case, the F-statistic is 12.59, with degrees of freedom of 2 for the numerator and 997 for the denominator.\nThe p-value associated with the F-statistic is very small (p-value: 3.996e-06), which indicates that the overall model is statistically significant.\n\nIn summary, the regression model has a statistically significant overall relationship with the response variable.\nThe variable hrs_personal_dev has a significant positive relationship with the response variable, while hrs_working does not have a significant relationship.\nHowever, it's important to note that the model's R-squared value is relatively low, suggesting that the predictor variables explain only a small portion of the variance in the response variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlin_model2_IQ <- lm(JP_Clean$client_sat ~ JP_Clean$IQ + JP_Clean$hrs_personal_dev, data = JP_Clean)\n\n\nsummary(lin_model2_IQ)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = JP_Clean$client_sat ~ JP_Clean$IQ + JP_Clean$hrs_personal_dev, \n    data = JP_Clean)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-59.348 -12.099  -0.457  12.084  45.872 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                10.6247    14.7768   0.719   0.4723    \nJP_Clean$IQ                 0.3861     0.1510   2.557   0.0107 *  \nJP_Clean$hrs_personal_dev   1.1064     0.2261   4.894 1.15e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.75 on 997 degrees of freedom\nMultiple R-squared:  0.03027,\tAdjusted R-squared:  0.02832 \nF-statistic: 15.56 on 2 and 997 DF,  p-value: 2.216e-07\n```\n:::\n:::\n\n\nResiduals: Residuals represent the differences between the observed values of the dependent variable and the predicted values obtained from the regression model.\nIn this case, the minimum residual is -59.348, the 25th percentile (1Q) is -12.099, the median (50th percentile) is -0.457, the 75th percentile (3Q) is 12.084, and the maximum residual is 45.872.\n\nCoefficients: The coefficients show the estimated effects of the independent variables on the dependent variable.\nThe three coefficients are as follows:\n\nThe coefficient for the constant term (Intercept) is 10.6247.\nThe coefficient for the variable IQ is 0.3861.\nThe coefficient for the variable hrs_personal_dev is 1.1064.\n\nStandard Errors: Standard errors indicate the variability of the estimated coefficients.\nA lower standard error suggests a more precise estimate.\n\nFor example, the standard error for the Intercept is 14.7768, for IQ is 0.1510, and hrs_personal_dev is 0.2261.\n\nt-values: The t-values are calculated by dividing the coefficient estimates by their respective standard errors.\nIt indicates the number of standard deviations the coefficient is away from zero.\nLarger t-values imply stronger evidence against the null hypothesis (that the true coefficient is zero).\nIn this case:\n\nThe t-value for the Intercept is 0.719.\nThe t-value for IQ is 2.557.\nThe t-value for JP_Clean\\$hrs_personal_dev is 4.894.\np-values: The p-values are associated with the t-values and help determine the statistical significance of the coefficients.\nLower p-values (typically below 0.05) suggest that the corresponding variable has a statistically significant effect on the dependent variable.\nIn this case:\n\nThe p-value for the Intercept is 0.4723.\nThe p-value for IQ is 0.0107 (indicated by -\\> \\* ... The p-value for hrs_personal_dev is 1.15e-06 (indicated by \\*\\*\\*).\nInterpretation of the significant variables:\n\nThe variable IQ has a statistically significant effect on the dependent variable at a significance level of 0.05 (since the p-value is less than 0.05).\nFor each unit increase in \"IQ,\" the dependent variable is expected to increase by approximately 0.3861 units, all other factors being constant.\nThe variable personal_dev also has a statistically significant effect on the dependent variable at a significance level much lower than 0.001 (represented by \\*\\*\\*).\nFor each additional hour spent on \"personal development\" (assuming other variables are constant), the dependent variable is expected to increase by approximately 1.1064 units.\nResidual Standard Error: The residual standard error (RSE) measures the average magnitude of the residuals, indicating how well the model fits the data.\nIn this case, the RSE is 17.75.\n\nMultiple R-squared and Adjusted R-squared: These measures represent the goodness of fit of the model.\nThey indicate the proportion of the variance in the dependent variable that is explained by the independent variables.\nThe multiple R-squared is 0.03027, which means the independent variables collectively explain about 3.03% of the variance in the dependent variable.\nThe adjusted R-squared (0.02832) considers the number of predictors in the model and penalizes for including irrelevant predictors.\n\nF-statistic and p-value: The F-statistic tests the overall significance of the model.\nA low p-value (smaller than your chosen significance level, often 0.05) indicates that the model as a whole is statistically significant.\nIn this case, the F-statistic is 15.56, and the p-value is 2.216e-07 (very close to zero), indicating that the model is statistically significant.\n\n## Correlation -- Switching to defer to hrs_personal_dev in light of significance\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Using the Boston data set from the \n\ncor_matrix <- cor(JP_Clean)\nprint(cor_matrix)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                           years_edu          IQ hrs_personal_dev hrs_working\nyears_edu                 1.00000000 0.463767221      -0.05287055 -0.05052624\nIQ                        0.46376722 1.000000000       0.02454638  0.01361938\nhrs_personal_dev         -0.05287055 0.024546376       1.00000000  0.81632958\nhrs_working              -0.05052624 0.013619381       0.81632958  1.00000000\nclient_sat                0.17344725 0.083505580       0.15463409  0.11078442\nsuper_sat                 0.03663007 0.135297608       0.68254571  0.82337833\nsuccess_project_complete  0.02100651 0.052836533       0.79096362  0.97640689\navg_psych_score           0.04376263 0.007437749      -0.04506696 -0.06328316\n                         client_sat  super_sat success_project_complete\nyears_edu                0.17344725 0.03663007               0.02100651\nIQ                       0.08350558 0.13529761               0.05283653\nhrs_personal_dev         0.15463409 0.68254571               0.79096362\nhrs_working              0.11078442 0.82337833               0.97640689\nclient_sat               1.00000000 0.39055769               0.11921314\nsuper_sat                0.39055769 1.00000000               0.80896311\nsuccess_project_complete 0.11921314 0.80896311               1.00000000\navg_psych_score          0.73650972 0.44354900              -0.06255367\n                         avg_psych_score\nyears_edu                    0.043762626\nIQ                           0.007437749\nhrs_personal_dev            -0.045066963\nhrs_working                 -0.063283161\nclient_sat                   0.736509715\nsuper_sat                    0.443549003\nsuccess_project_complete    -0.062553669\navg_psych_score              1.000000000\n```\n:::\n:::\n\n\nThese results are very interesting.\nAlthough it turned out hours worked didn't improve client satisfaction, it appears to improve superior satisfaction.\nI suppose that makes a bit of sense, as the more hours your boss sees you putting in, the more it appears you care, and the happier they are.\nIt's a VERY strong correlation at 0.82.\n\nAdditionally, it appears client satisfaction is correlated rather highly (0.74) to average pysch score, implying that the higher a pysch score is, the happier a client will be.\n\nThere is concern of multicolineaity in my model, or at least the prospect of its presence.\nHours of personal dev and hours working are highly positively correlated at .816\n\n## Linear for Graph\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimple_lin <- (lm(JP_Clean$hrs_personal_dev ~ JP_Clean$client_sat ))\n\nsimple_lin\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = JP_Clean$hrs_personal_dev ~ JP_Clean$client_sat)\n\nCoefficients:\n        (Intercept)  JP_Clean$client_sat  \n            4.85505              0.02134  \n```\n:::\n\n```{.r .cell-code}\nrev_lin <- (lm(JP_Clean$client_sat ~ JP_Clean$hrs_personal_dev ))\n\nrev_lin\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = JP_Clean$client_sat ~ JP_Clean$hrs_personal_dev)\n\nCoefficients:\n              (Intercept)  JP_Clean$hrs_personal_dev  \n                   48.215                      1.121  \n```\n:::\n:::\n\n\nIntercept (B0): The intercept is approximately 48.215.\nThis represents the expected value of the client_sat rating when the hrs_personal_dev is 0, assuming all other predictor variables are held constant.\n\nCoefficient for hrs_personal_dev (B1): The coefficient for hrs_personal_dev is approximately 1.121.\nThis means that for every additional unit increase in hrs_personal_dev, the client_sat rating is expected to increase by about 1.121 points, assuming all other predictor variables are constant.\n\nThe coefficient for hrs_personal_dev is positive, which indicates a positive linear relationship between the client_sat rating and hrs_personal_dev.\nThis suggests that as individuals spend more hours on personal development, their reported client satisfaction tends to increase.\n\n## log and cubic\n\n#### log\n\n\n::: {.cell}\n\n```{.r .cell-code}\nJP_Clean <- na.omit(JP_Clean)\nJP_Clean$client_sat[JP_Clean$client_sat == 0] <- 0.001\n\nJP_Clean <- JP_Clean[complete.cases(JP_Clean) & JP_Clean$hrs_personal_dev > 0, ]\n\nLinearlog_model <- lm(hrs_personal_dev ~ log(client_sat), data = JP_Clean)\n\nLinearlog_model\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = hrs_personal_dev ~ log(client_sat), data = JP_Clean)\n\nCoefficients:\n    (Intercept)  log(client_sat)  \n          4.138            0.487  \n```\n:::\n\n```{.r .cell-code}\nlin_rev <- lm(client_sat ~ log(hrs_personal_dev), data = JP_Clean)\n\nlin_rev\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = client_sat ~ log(hrs_personal_dev), data = JP_Clean)\n\nCoefficients:\n          (Intercept)  log(hrs_personal_dev)  \n               45.375                  5.617  \n```\n:::\n:::\n\n\nIntercept (45.375):\n\nThe intercept term is 45.375. This represents the estimated level of client satisfaction when the natural logarithm of hours spent on personal development is zero. \n\nLog(Hours on Personal Development) Coefficient (5.617):\n\nThe coefficient of 5.617 indicates that a one-unit increase in the natural logarithm of hours spent on personal development is associated with an estimated increase of 5.617 units in client satisfaction which is rather large.\n\nBecause the independent variable is the logarithm of hours spent on personal development, a change in the logarithm corresponds to a multiplicative change in the dependent variable. In this case, for each percentage increase (or decrease) in hours spent on personal development, there's an expected multiplicative increase (or decrease) of approximately 561.7% in client satisfaction.\n\n\n#### Log-Log\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#log log will make the lines and points line up more efficiently, leave client_sat alone.\n\nmodelll <- lm(log(hrs_personal_dev) ~ log(client_sat), data = JP_Clean)\n\nsummary(modelll)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = log(hrs_personal_dev) ~ log(client_sat), data = JP_Clean)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.75252 -0.30529  0.06882  0.32283  1.10057 \n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      1.36296    0.08767  15.547  < 2e-16 ***\nlog(client_sat)  0.08792    0.02208   3.982 7.33e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4499 on 995 degrees of freedom\nMultiple R-squared:  0.01569,\tAdjusted R-squared:  0.0147 \nF-statistic: 15.86 on 1 and 995 DF,  p-value: 7.333e-05\n```\n:::\n:::\n\n\n#### Cubic\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncubic_model <- lm(hrs_personal_dev ~ I(client_sat) + I(client_sat^2) + I(client_sat^3), data =JP_Clean)\n\ncubic_model\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = hrs_personal_dev ~ I(client_sat) + I(client_sat^2) + \n    I(client_sat^3), data = JP_Clean)\n\nCoefficients:\n    (Intercept)    I(client_sat)  I(client_sat^2)  I(client_sat^3)  \n      4.735e+00        1.496e-02        3.756e-04       -3.499e-06  \n```\n:::\n\n```{.r .cell-code}\ncub_rev <- lm(client_sat ~ I(hrs_personal_dev) + I(hrs_personal_dev^2) + I(hrs_personal_dev^3), data =JP_Clean)\n\ncub_rev\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = client_sat ~ I(hrs_personal_dev) + I(hrs_personal_dev^2) + \n    I(hrs_personal_dev^3), data = JP_Clean)\n\nCoefficients:\n          (Intercept)    I(hrs_personal_dev)  I(hrs_personal_dev^2)  \n            51.169883              -0.347459               0.199920  \nI(hrs_personal_dev^3)  \n            -0.007707  \n```\n:::\n:::\n\n\nIntercept (Bo): The intercept is approximately 51.17.\nThis represents the expected value of the client_sat rating when the hrs_personal_dev is 0, assuming all other predictor variables are held constant.\n\n\nLinear Coefficient (B1): The linear coefficient is approximately -0.3475.\nThis means that for every additional unit increase in hrs_personal_dev, the client_sat rating is expected to decrease by about 0.3475 points, assuming all other predictor variables are constant.\n\nQuadratic Coefficient (B2): The quadratic coefficient is approximately 0.1999.\nThis suggests that the rate of change of client_sat with respect to hrs_personal_dev changes by about 0.1999 units for each additional unit increase in hrs_personal_dev.\nThe positive value indicates that the curve is concave upward (starts to flatten out).\n\nCubic Coefficient (B3): The cubic coefficient is approximately -0.007707.\nThis coefficient captures how the curvature of the relationship between hrs_personal_dev and client_sat changes.\nA negative value suggests that the rate of change of the curve's slope is decreasing, indicating a potential inflection point in the relationship.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(JP_Clean$client_sat, JP_Clean$hrs_personal_dev,\n     pch = 20,\n     col = \"pink3\",\n     xlab = \"Client Satisfaction\",\n     ylab = \"Hours of Personal Development\",\n     xlim = c(0, 100),\n     ylim = c(0, 40))\n\nabline(simple_lin, lwd = 2)\nabline(modelll, lwd = 2)\n\n\norder_id <- order(JP_Clean$hrs_personal_dev)\n\nlines(JP_Clean$hrs_personal_dev[order_id],\nfitted(cubic_model)[order_id],\ncol = \"red\",\nlwd = 2)\n\nlegend(\"topright\",\nlegend = c(\"Linear-Hrs Development\", \"Log-Log Hrs Development\", \"Cubic-Hrs Development\"),\nlty = 1,\ncol = c( \"red\", \"orange\", \"blue\"))\n```\n\n::: {.cell-output-display}\n![](Landreth_Final_603_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n```{.r .cell-code}\n#??????\n```\n:::\n\n\n## Model Specification\n\n\n::: {.cell}\n\n```{.r .cell-code}\nTestsat_mod1 <- lm(client_sat ~ hrs_personal_dev, data = JP_Clean)\n\nTestsat_mod2 <- lm(client_sat ~ hrs_personal_dev + IQ + success_project_complete,\ndata = JP_Clean)\n\nTestsat_mod3 <- lm(client_sat ~ hrs_personal_dev + IQ + success_project_complete + hrs_working + I(hrs_working^2) + I(hrs_working^3), data = JP_Clean)\n\nTestsat_mod4 <- lm(client_sat ~ hrs_personal_dev + I(hrs_personal_dev^2) + I(hrs_personal_dev^3) + IQ + success_project_complete\n+ hrs_working + I(hrs_working^2) + I(hrs_working^3), data = JP_Clean)\n\nTestsat_mod5 <- lm(client_sat ~ hrs_personal_dev + years_edu + I(hrs_working^2) + I(hrs_working^3) + years_edu:hrs_personal_dev + IQ + hrs_working, data = JP_Clean)\n\nTestsat_mod6 <- lm(client_sat ~ hrs_personal_dev + I(hrs_working^2) + I(hrs_working^3)+ IQ + hrs_working, data = JP_Clean)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#install.packages(\"sandwich\")\n\nrob_se <- list(sqrt(diag(vcovHC(Testsat_mod1, type = \"HC1\"))),\nsqrt(diag(vcovHC(Testsat_mod2, type = \"HC1\"))),\nsqrt(diag(vcovHC(Testsat_mod3, type = \"HC1\"))),\nsqrt(diag(vcovHC(Testsat_mod4, type = \"HC1\"))),\nsqrt(diag(vcovHC(Testsat_mod5, type = \"HC1\"))),\nsqrt(diag(vcovHC(Testsat_mod6, type = \"HC1\"))))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a vector to hold the robust standard errors\nmodel_list <- list(\n  \"Model 1\" = Testsat_mod1,\n  \"Model 2\" = Testsat_mod2,\n  \"Model 3\" = Testsat_mod3,\n  \"Model 4\" = Testsat_mod4,\n  \"Model 5\" = Testsat_mod5,\n  \"Model 6\" = Testsat_mod6\n)\n\n# Use the named list in the stargazer function\nstargazer(\n  models = model_list,\n  title = \"Regressions Using Job Performance Data\",\n  type = \"latex\",\n  digits = 3,\n  header = FALSE,\n  se = rob_se,  # Make sure rob_se is defined correctly\n  object.names = TRUE,\n  model.numbers = FALSE,\n  column.labels = c(\"(I)\", \"(II)\", \"(III)\", \"(IV)\", \"(V)\", \"(VI)\")\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\\begin{table}[!htbp] \\centering \n  \\caption{Regressions Using Job Performance Data} \n  \\label{} \n\\begin{tabular}{@{\\extracolsep{5pt}}lcccccc} \n\\\\[-1.8ex]\\hline \n\\hline \\\\[-1.8ex] \n & \\multicolumn{6}{c}{\\textit{Dependent variable:}} \\\\ \n\\cline{2-7} \n\\\\[-1.8ex] & \\multicolumn{6}{c}{client\\_sat} \\\\ \n & (I) & (II) & (III) & (IV) & (V) & (VI) \\\\ \n\\\\[-1.8ex] & models = model_list & NA & NA & NA & NA & NA\\\\ \n\\hline \\\\[-1.8ex] \n hrs\\_personal\\_dev & 1.145$^{***}$ & 1.237$^{***}$ & 1.357$^{***}$ & 2.699 & 1.490$^{***}$ & 1.331$^{***}$ \\\\ \n  & (0.226) & (0.384) & (0.418) & (2.687) & (0.508) & (0.417) \\\\ \n  & & & & & & \\\\ \n I(hrs\\_personal\\_dev$\\hat{\\mkern6mu}$2) &  &  &  & $-$0.130 &  &  \\\\ \n  &  &  &  & (0.352) &  &  \\\\ \n  & & & & & & \\\\ \n I(hrs\\_personal\\_dev$\\hat{\\mkern6mu}$3) &  &  &  & 0.002 &  &  \\\\ \n  &  &  &  & (0.014) &  &  \\\\ \n  & & & & & & \\\\ \n years\\_edu &  &  &  &  & 2.297$^{***}$ &  \\\\ \n  &  &  &  &  & (0.849) &  \\\\ \n  & & & & & & \\\\ \n IQ &  & 0.390$^{**}$ & 0.320$^{*}$ & 0.322$^{*}$ & $-$0.070 & 0.364$^{**}$ \\\\ \n  &  & (0.164) & (0.165) & (0.166) & (0.174) & (0.164) \\\\ \n  & & & & & & \\\\ \n success\\_project\\_complete &  & $-$0.016 & 0.193 & 0.192 &  &  \\\\ \n  &  & (0.045) & (0.135) & (0.136) &  &  \\\\ \n  & & & & & & \\\\ \n hrs\\_working &  &  & $-$0.536$^{**}$ & $-$0.557$^{**}$ & $-$0.402$^{*}$ & $-$0.356$^{*}$ \\\\ \n  &  &  & (0.250) & (0.262) & (0.209) & (0.212) \\\\ \n  & & & & & & \\\\ \n hrs\\_personal\\_dev:years\\_edu &  &  &  &  & $-$0.048 &  \\\\ \n  &  &  &  &  & (0.132) &  \\\\ \n  & & & & & & \\\\ \n I(hrs\\_working$\\hat{\\mkern6mu}$2) &  &  & 0.006 & 0.006 & 0.007 & 0.006 \\\\ \n  &  &  & (0.005) & (0.005) & (0.005) & (0.005) \\\\ \n  & & & & & & \\\\ \n I(hrs\\_working$\\hat{\\mkern6mu}$3) &  &  & $-$0.00003 & $-$0.00002 & $-$0.00003 & $-$0.00003 \\\\ \n  &  &  & (0.00003) & (0.00003) & (0.00003) & (0.00003) \\\\ \n  & & & & & & \\\\ \n Constant & 48.043$^{***}$ & 10.180 & 20.189 & 17.333 & 54.310$^{***}$ & 17.351 \\\\ \n  & (1.520) & (16.142) & (16.311) & (17.052) & (17.186) & (16.296) \\\\ \n  & & & & & & \\\\ \n\\hline \\\\[-1.8ex] \nObservations & 997 & 997 & 997 & 997 & 997 & 997 \\\\ \nR$^{2}$ & 0.025 & 0.031 & 0.038 & 0.039 & 0.064 & 0.036 \\\\ \nAdjusted R$^{2}$ & 0.024 & 0.028 & 0.032 & 0.031 & 0.058 & 0.031 \\\\ \nResidual Std. Error & 17.815 (df = 995) & 17.773 (df = 993) & 17.738 (df = 990) & 17.748 (df = 988) & 17.501 (df = 989) & 17.749 (df = 991) \\\\ \nF Statistic & 25.013$^{***}$ (df = 1; 995) & 10.598$^{***}$ (df = 3; 993) & 6.481$^{***}$ (df = 6; 990) & 4.965$^{***}$ (df = 8; 988) & 9.712$^{***}$ (df = 7; 989) & 7.326$^{***}$ (df = 5; 991) \\\\ \n\\hline \n\\hline \\\\[-1.8ex] \n\\textit{Note:}  & \\multicolumn{6}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\\\ \n\\end{tabular} \n\\end{table} \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nTestsat_mod1 <- lm(client_sat ~ I(hrs_personal_dev) + I(hrs_personal_dev^2) + I(hrs_personal_dev^3), data = JP_Clean)\n\n# Test a linear hypothesis for squared term\nhypothesis_result_sq_1 <- linearHypothesis(Testsat_mod1, \"I(hrs_personal_dev^2) = 0\")\n\n# Test a linear hypothesis for cubed term\nhypothesis_result_cube_1 <- linearHypothesis(Testsat_mod1, \"I(hrs_personal_dev^3) = 0\")\n\nhypothesis_result_sq_1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear hypothesis test\n\nHypothesis:\nI(hrs_personal_dev^2) = 0\n\nModel 1: restricted model\nModel 2: client_sat ~ I(hrs_personal_dev) + I(hrs_personal_dev^2) + I(hrs_personal_dev^3)\n\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\n1    994 315759                           \n2    993 315642  1    116.71 0.3672 0.5447\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nTestsat_mod2 <- lm(client_sat ~ I(hrs_working) + I(hrs_working^2) + I(hrs_working^3), data = JP_Clean)\n\n# Test a linear hypothesis for squared term\nhypothesis_result_sq_2 <- linearHypothesis(Testsat_mod2, \"I(hrs_working^2) = 0\")\n\n# Test a linear hypothesis for cubed term\nhypothesis_result_cube_2 <- linearHypothesis(Testsat_mod2, \"I(hrs_working^3) = 0\")\n\n\nhypothesis_result_sq_2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear hypothesis test\n\nHypothesis:\nI(hrs_working^2) = 0\n\nModel 1: restricted model\nModel 2: client_sat ~ I(hrs_working) + I(hrs_working^2) + I(hrs_working^3)\n\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\n1    994 317650                           \n2    993 317398  1    251.19 0.7859 0.3756\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nTestsat_mod3 <- lm(client_sat ~ I(IQ) + I(IQ^2) + I(IQ^3), data = JP_Clean)\n\n# Test a linear hypothesis for squared term\nhypothesis_result_sq_3 <- linearHypothesis(Testsat_mod3, \"I(IQ^2) = 0\")\n\n# Test a linear hypothesis for cubed term\nhypothesis_result_cube_3 <- linearHypothesis(Testsat_mod3, \"I(IQ^3) = 0\")\n\nhypothesis_result_sq_3\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear hypothesis test\n\nHypothesis:\nI(IQ^2) = 0\n\nModel 1: restricted model\nModel 2: client_sat ~ I(IQ) + I(IQ^2) + I(IQ^3)\n\n  Res.Df    RSS Df Sum of Sq      F  Pr(>F)  \n1    994 320889                              \n2    993 319269  1    1620.3 5.0397 0.02499 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nTestsat_mod4 <- lm(client_sat ~ I(years_edu) + I(years_edu^2) + I(years_edu^3), data = JP_Clean)\n\n# Test a linear hypothesis for squared term\nhypothesis_result_sq_4 <- linearHypothesis(Testsat_mod4, \"I(years_edu^2) = 0\")\n\n# Test a linear hypothesis for cubed term\nhypothesis_result_cube_4 <- linearHypothesis(Testsat_mod4, \"I(years_edu^3) = 0\")\n\nhypothesis_result_sq_4\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear hypothesis test\n\nHypothesis:\nI(years_edu^2) = 0\n\nModel 1: restricted model\nModel 2: client_sat ~ I(years_edu) + I(years_edu^2) + I(years_edu^3)\n\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\n1    994 313729                           \n2    993 313398  1     330.5 1.0472 0.3064\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nTestsat_mod5 <- lm(client_sat ~ I(success_project_complete) + I(success_project_complete^2) + I(success_project_complete^3), data = JP_Clean)\n\n# Test a linear hypothesis for squared term\nhypothesis_result_sq_5 <- linearHypothesis(Testsat_mod5, \"I(success_project_complete^2) = 0\")\n\n# Test a linear hypothesis for cubed term\nhypothesis_result_cube_5 <- linearHypothesis(Testsat_mod5, \"I(success_project_complete^3) = 0\")\n\nhypothesis_result_sq_5\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear hypothesis test\n\nHypothesis:\nI(success_project_complete^2) = 0\n\nModel 1: restricted model\nModel 2: client_sat ~ I(success_project_complete) + I(success_project_complete^2) + \n    I(success_project_complete^3)\n\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\n1    994 317571                           \n2    993 317536  1    34.364 0.1075 0.7431\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nTestsat_mod6 <- lm(client_sat ~ I(avg_psych_score) + I(avg_psych_score^2) + I(avg_psych_score^3), data = JP_Clean)\n\n# Test a linear hypothesis for squared term\nhypothesis_result_sq_6 <- linearHypothesis(Testsat_mod6, \"I(avg_psych_score^2) = 0\")\n\n# Test a linear hypothesis for cubed term\nhypothesis_result_cube_6 <- linearHypothesis(Testsat_mod6, \"I(avg_psych_score^3) = 0\")\n\nhypothesis_result_sq_6\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear hypothesis test\n\nHypothesis:\nI(avg_psych_score^2) = 0\n\nModel 1: restricted model\nModel 2: client_sat ~ I(avg_psych_score) + I(avg_psych_score^2) + I(avg_psych_score^3)\n\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\n1    994 148050                           \n2    993 147933  1     116.7 0.7834 0.3763\n```\n:::\n:::\n\n\n## Explanation\n\nModel 1 (Testsat_mod1):\n\nThe hypothesis tests for the squared and cubed terms of hrs_personal_dev:\n\nFor the squared term (I(hrs_personal_dev\\^2)): The p-value is 0.5447, which is greater than the commonly used significance level (e.g., 0.05).\nThis suggests that we do not have sufficient evidence to reject the null hypothesis that the squared term's coefficient is equal to zero.\nFor the cubed term (I(hrs_personal_dev\\^3)): The p-value is not provided in the output, but if it's not shown, it's likely above 0.05 as well.\nModel 2 (Testsat_mod2):\n\nThe hypothesis tests for the squared and cubed terms of hrs_working:\n\nFor the squared term (I(hrs_working\\^2)): The p-value is 0.3756, which is greater than 0.05.\nThis suggests that we do not have sufficient evidence to reject the null hypothesis that the squared term's coefficient is equal to zero.\nFor the cubed term (I(hrs_working\\^3)): The p-value is not provided in the output, but if it's not shown, it's likely above 0.05 as well.\nModel 3 (Testsat_mod3):\n\nThe hypothesis tests for the squared and cubed terms of IQ:\n\nFor the squared term (I(IQ\\^2)): The p-value is 0.02499, which is less than 0.05.\nThis suggests that we have evidence to reject the null hypothesis that the squared term's coefficient is equal to zero.\nIn other words, the squared term of IQ is statistically significant in explaining the variation in client_sat.\nFor the cubed term (I(IQ\\^3)): The p-value is not provided in the output, but if it's not shown, it's likely above 0.05.\nModel 4 (Testsat_mod4):\n\nThe hypothesis tests for the squared and cubed terms of years_edu:\n\nFor the squared term (I(years_edu\\^2)): The p-value is 0.3064, which is greater than 0.05.\nThis suggests that we do not have sufficient evidence to reject the null hypothesis that the squared term's coefficient is equal to zero.\nFor the cubed term (I(years_edu\\^3)): The p-value is not provided in the output, but if it's not shown, it's likely above 0.05.\nModel 5 (Testsat_mod5):\n\nThe hypothesis tests for the squared and cubed terms of success_project_complete:\n\nFor the squared term (I(success_project_complete\\^2)): The p-value is 0.7431, which is greater than 0.05.\nThis suggests that we do not have sufficient evidence to reject the null hypothesis that the squared term's coefficient is equal to zero.\nFor the cubed term (I(success_project_complete\\^3)): The p-value is not provided in the output, but if it's not shown, it's likely above 0.05.\nModel 6 (Testsat_mod6):\n\nThe hypothesis tests for the squared and cubed terms of avg_psych_score:\n\nFor the squared term (I(avg_psych_score\\^2)): The p-value is 0.3763, which is greater than 0.05.\nThis suggests that we do not have sufficient evidence to reject the null hypothesis that the squared term's coefficient is equal to zero.\nFor the cubed term (I(avg_psych_score\\^3)): The p-value is not provided in the output, but if it's not shown, it's likely above 0.05.\n\nAll this being said, shockingly, the hypothesis test for the squared term of IQ (I(IQ\\^2)) yielded a p-value of 0.02499, which is less than the common significance level of 0.05.\nThis indicates that the squared term of IQ is statistically significant in explaining the variation in client satisfaction.\n\nWhile the p-value for the cubed term of IQ (I(IQ\\^3)) is not provided in the output, the significance of the squared term suggests that the model is capturing a nonlinear relationship between IQ and client_sat.\n\nIn contrast, the other models (Models 1, 2, 4, 5, and 6) did not show statistically significant squared or cubed terms based on their respective hypothesis tests.\n",
    "supporting": [
      "Landreth_Final_603_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}